---
layout: post
title: "DeepSeek Code Reading and Study 01"
categories: GPU, LLM, Distributed
hidden: true
---

## DeepSeek's Fire-Flyer File System (3FS)

[3FS](https://github.com/deepseek-ai/3FS) is a distributed file system designed specifically for AI workloads, optimized for read-heavy scenarios and large scale metadata management. It aims to support various stages including training data preprocessing, dataset loading, checkpointing, KV cache for inference, and [embedding vector search](https://learn.microsoft.com/en-us/azure/search/vector-search-overview).


## System Architecture

The core components of 3FS includes

- Cluster Manager: Manages the overall cluster state and FO (Failover). Other components interact with the Cluster Manager through heartbeats. The Cluster Manager ensures its own reliability via a multi-replica leader election mechanism, using Zookeeper (ZK) or ETCD

- Metadata Service: Stores files metadata, In 3FS, metadata is stored in an independent/external FoundationDB (FDB) cluster. The data reliability of metadata is guaranteed by FDB, which is a complete distributed key-value database

- Storage Service: Stores data using the [CRAQ (Chain Replication with Appointed Queries)](https://www.usenix.org/legacy/event/usenix09/tech/full_papers/terrace/terrace.pdf) for data synchronization and ensures high availability through its own mechanisms

- Client: Uses FUSE clients for performance-insensitive applications and native clients for high performance requirements


Here is the full translation of the document into English:

---

### External Dependencies

- **ClickHouse**: Used to store metrics generated by the storage service.
- **FoundationDB**: Used by the metadata service to store file metadata.
- **Zookeeper/etcd**: Used by the Cluster Manager to implement multi-replica leader election.

---

### Metadata Service (MetaService)

#### Design

Similar to [ADLS](https://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-introduction), 3FS uses FoundationDB, that supports [SSI (Serializable Snapshot Isolation)](https://www.foundationdb.org/files/fdb-paper.pdf) level transactions (similar to 2PL implementations with serializable isolation). Using SSI transactions significantly reduces the cost of designing and implementing a file system directory tree. The complexity is offloaded to the database since SSI transactions are ACID-compliant, particularly in terms of consistency. There are no external consistency issues, as all transactions are semantically equivalent to being executed one at a time in sequence. Each directory tree operation is transaction-based, ensuring there are no consistency issues. This eliminates anomalies like cyclic renaming issues seen in non-SSI implementations.

With SSI transactions maintaining state within the database, 3FS's MetaService becomes stateless. All MetaService instances act as interchangeable proxies at the directory tree structure level. Notably, 3FS could eliminate the MetaService layer entirely and handle metadata requests directly on the client side, shortening request paths. However, retaining MetaService has advantages: bugs in metadata processing logic can be fixed without requiring client upgrades, and it prevents excessive database sessions caused by too many clients.

#### Implementation

Metadata operations leverage FoundationDB's SSI transactions:

- **Read-only transactions**: Used for metadata queries like `fstat`, `lookup`, and `listdir`.
- **Read-write transactions**: Used for metadata updates like `create`, `link`, `unlink`, and `rename`.

All consistency complexities are handled by FoundationDB. For example, if a cyclic rename issue arises, transaction conflicts will cause one transaction to be canceled, and 3FS's metadata service will automatically retry the transaction. This design pattern is common for small teams working on storage and database systems.

#### Insights

3FS's metadata service architecture aligns with systems like HopsFS, Tectonic, and Baidu CFS. Considering that POSIX file systems inherently involve transactional properties (e.g., cyclic rename issues or orphan nodes from concurrent `create` and `unlink` operations), earlier metadata services were often proprietary systems that struggled with cross-shard transaction issues. These newer systems offload such problems to distributed databases and implement stateless metadata services, achieving horizontal scalability while maintaining transactional properties.

The choice of FoundationDB is due to its support for SSI-level transaction isolation and its simple yet efficient key-value interface. The implementation organizes Inode and Directory as separate record types within the key-value schema.

From 3FS's source code, many design elements of MetaService closely resemble Baidu CFS, especially in choosing FoundationDB. This reflects significant experience and work by the team. Systems like HopsFS often find traditional SQL parsing unnecessary for their use cases, making a strongly consistent distributed key-value store like FoundationDB more suitable. However, FoundationDB's optimistic transaction model can degrade performance under high transaction conflict scenarios. To address this, 3FS separates Inode and Dentry (Directory Entry) schemas—similar to Baidu CFS—to optimize for these conflicts.

Key design choices include:
- **Little-endian encoding for Inode IDs**: Ensures even distribution of metadata requests across shards.
- **Parent ID in Inode attributes**: Facilitates cyclic rename detection.

Using an external database like FoundationDB offers additional benefits:
1. **Customizable metadata interfaces for better performance**: For example, Baidu's Tafdb implements highly optimized one-phase commit primitives instead of simple key-value interfaces.
2. **Directory-tree-friendly balancing strategies**: Adaptive splitting strategies for data lakes can be implemented.

---

### Chunk Placement

Like GFS, 3FS does not maintain a chunk list in metadata. Chunk IDs and locations are computed dynamically:

- **Chunk ID**: Since chunks are fixed-size, their position (`chunk_index`) can be calculated using `offset / chunk_size`. The chunk ID is derived from `{ino}{chunk_index}`.
- **Chunk location routing**: Each Inode is assigned a `chain_table` and `shuffle_seed` upon creation. Both values remain constant. To determine a chunk's location, only `chunk_index`, `shuffle_seed`, and `chain_table` are required.

---

### Data Structures

| Metadata | Key | Value | Description |
|----------|-----|-------|-------------|
| Dentry   | DENT{parent_ino}{name} | {parent_ino}{ino}{chain_table}{chunk_size}... | Prefixes like DENT/INOD isolate data in FoundationDB’s global KV space. |
| Inode    | INOD{ino} | {file_length}{chunk_size}{shuffle_seed}... | Chunk lists are not maintained; chunk indices are derived from offsets.|

---

### Dynamic File Attributes

1. **Delayed file deletion**: POSIX requires files to remain accessible even after being unlinked if they are open. 3FS delays deletion of open files but does not fully implement POSIX semantics.
2. **Lazy file length updates**: Clients periodically report maximum write positions to MetaService (default every 5 seconds).

---

### Analysis

The overall design aligns with Microsoft's ADLS by leveraging serializable isolation-level transactions for directory tree consistency. Unlike systems like Baidu CFS or DanceNN v3 that rely on low-isolation-level transactions with application-layer locks, or HopsFS that uses RC-level isolation requiring app-layer lock management, 3FS simplifies development by offloading complexity to the database.

---

### Chunk Storage Service

3FS uses striping similar to Ceph but employs CRAQ (Chain Replication with Apportioned Quorums) for consistency:

- **Write operations**: Data is written sequentially through chain nodes.
- **Read operations**: Reads can occur on any node but require validation from the tail node for dirty data.

Potential issues:
- Lower availability compared to quorum-based star writes (e.g., GPFS).
- Challenges with online erasure coding (EC).

---

### Fault Handling

Each chain maintains:
1. Local state (e.g., lease status).
2. Global state managed by ClusterManager.

Faulty replicas are moved out of chains until recovery completes.

---

### File System Interface

3FS supports two usage modes:
1. **POSIX API**: Chosen over object storage interfaces due to directory atomic operations, symlinks/hardlinks, and compatibility with existing file formats.
2. **Native Client API**: For high-performance applications.

---

### FUSE Implementation

3FS uses user-space FUSE instead of kernel modules for simplicity:
- Control paths use standard libfuse.
- Data paths use shared memory via InfiniBand (IB) for zero-copy data transfers.

Advantages:
1. Simplified development.
2. Shared caching across clients.
3. Improved cold-start performance after model restarts.

---

### AI Workload Optimization

For AI workloads like checkpointing:
1. Parameters/optimizer files are split into smaller chunks.
2. Data transfer from GPU to CPU is asynchronous.
3. Metadata processing is parallelized to minimize training disruptions.

---

### Summary

3FS is an AI-focused storage system optimized for read-heavy workloads with features like zero-copy data paths (via RDMA), scalable metadata handling using FoundationDB, and support for concurrent writes/overwrites.

Its greatest strength lies in co-designing storage with AI frameworks and networks to maximize training efficiency.

--- 



## Data Storage

- Utilizes CRAQ for data replication, offering good read performance with some trade-offs in write latency

- implements a chain-based replication strategy similar to Ceph, with predefined replication groupds (CopySets)

- Employs a clever schema design to optimize performance and reduce transaction conflicts

## Client Interface

- Provides both POSIX API and Native Client API options 
  
- Implements a user-space [FUSE(Filesystem in Userspace)](https://www.usenix.org/system/files/conference/fast17/fast17-vangoor.pdf) [client](https://github.com/libfuse/libfuse) with optimization for control and data paths

- Uses shared memory and InfiniBand for zero-copy data transfer in the native client

## AI workload optimization

- Supports high-performance checkpointing with file chunking and asynchronous operations

- Achieves single-node throughput of up to 10GB/s for I/O operations

- Implements co-design optimizations with training frameworks and networking components

## Advantages

- Zero-copy data transfer using Infiniband or RDMA for excellent read performance

- Scalable metadata management supporting billions of files

- Support for concurrent and overwrite operations on files 

## Potential Limitations

- Write performance may be affected by the CRAQ protocol in certain scenarios

- Implementing online erasure coding can be challenging with the chain-based replication approach

3FS represents a comprehensive solution for AI-focused 

## Further reading

[Hierarchical Navigable Small Worlds (HNSW)](https://www.pinecone.io/learn/series/faiss/hnsw/)


- ZooKeeper is frequently used for [leader election](https://stackoverflow.com/questions/10732834/why-do-we-need-zookeeper-in-the-hadoop-stack) in [distributed systems](https://zookeeper.apache.org/doc/r3.4.5/zookeeperInternals.html). It provides a reliable and efficient mechanism for searching a [leader among multiple nodesin a cluster](https://hewi.blog/navigating-the-jungle-of-distributed-systems-a-guide-to-zookeeper-and-leader-election-algorithms)


- [Ceph](https://www.lightbitslabs.com/blog/ceph-storage/) is a free and open-source [software-defined storage](https://ceph.io/en/news/blog/2023/reef-osds-per-nvme/) platform serves as a back-end storage method for [OpenStack](https://en.wikipedia.org/wiki/OpenStack), supporting protocol allows clients (initiators) to send [NVMe commands to storage devices (targets)](https://docs.ceph.com/en/reef/rbd/nvmeof-overview/) over a TCP/IP network. 

- ETCD, a distributed key-value store that provides a reliable way to implement leader election in distributed systems
  
  - consistency: [etcd ensures that only one leader is elected at a time](https://etcd.io/docs/v3.6/tutorials/how-to-conduct-elections/)
  - Fault Tolerance: If a leader fails, a new election is automatically triggered


- [MIT 6.824 Lecture 9 CRAQ](https://timilearning.com/posts/mit-6.824/lecture-9-craq/)

## Appendix

### NVMe performance

NVMe is optimized for non-uniform memory access (NUMA) to allow for multiple CPU cores to manage queues

[NVMe Multipath](https://docs.kernel.org/admin-guide/nvme-multipath.html)

- NVMe supports multiple I/O queues, up to 64K with each queue having 64K entries. Efficient parallel processing across multiple CPU cores over SAS and SATA

- Intelligent I/O distribution/selecting paths closest to the NUMA node of the current CPU for optimal performance

- Scalability. In system with high core counts like modern AMD EPYC processors 

Facebook's [Folly](https://github.com/facebook/folly/tree/main) a library of C++17 components